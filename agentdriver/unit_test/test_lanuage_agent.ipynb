{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/agentdriver/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1i-JXtbOsvXuKyT1sWKX9utjpRuo1isrW\n",
      "From (redirected): https://drive.google.com/uc?id=1i-JXtbOsvXuKyT1sWKX9utjpRuo1isrW&confirm=t&uuid=c0fab215-2d3c-4fb3-8b09-bab3c406bf99\n",
      "To: /home/ubuntu/Ollama/Agent-Driver/data/finetune.zip\n",
      "100%|██████████████████████████████████████| 6.91M/6.91M [00:00<00:00, 8.45MB/s]\n",
      "/home/ubuntu/anaconda3/envs/agentdriver/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=14PWwG2oXkhOTY-hm8x3PiuKqZx7KKkSA\n",
      "From (redirected): https://drive.google.com/uc?id=14PWwG2oXkhOTY-hm8x3PiuKqZx7KKkSA&confirm=t&uuid=6c155574-6778-48ef-abe0-1aadf8d76f2a\n",
      "To: /home/ubuntu/Ollama/Agent-Driver/data/memory.zip\n",
      "100%|██████████████████████████████████████| 66.8M/66.8M [00:06<00:00, 11.1MB/s]\n",
      "/home/ubuntu/anaconda3/envs/agentdriver/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1_XkcaZDuEXQ-8Tz4LeGNoDGlLHbPW6aE\n",
      "From (redirected): https://drive.google.com/uc?id=1_XkcaZDuEXQ-8Tz4LeGNoDGlLHbPW6aE&confirm=t&uuid=7fb4bc3c-6839-4913-860f-85d5fbc80aac\n",
      "To: /home/ubuntu/Ollama/Agent-Driver/data/metrics.zip\n",
      "100%|██████████████████████████████████████| 19.6M/19.6M [00:02<00:00, 9.47MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 18vSSpL_52xVGEgK8vD_TcHFNLby0og0X -O ../../data/data.zip\n",
    "!gdown --id 1i-JXtbOsvXuKyT1sWKX9utjpRuo1isrW -O ../../data/finetune.zip\n",
    "!gdown --id 14PWwG2oXkhOTY-hm8x3PiuKqZx7KKkSA -O ../../data/memory.zip\n",
    "!gdown --id 1_XkcaZDuEXQ-8Tz4LeGNoDGlLHbPW6aE -O ../../data/metrics.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../../data/finetune.zip\n",
      "   creating: ../../data/finetune/\n",
      "  inflating: ../../data/finetune/data_samples_train.json  \n",
      "  inflating: ../../data/finetune/data_samples_val.json  \n",
      "Archive:  ../../data/memory.zip\n",
      "replace ../../data/memory/database.pkl? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "Archive:  ../../data/metrics.zip\n",
      "replace ../../data/metrics/uniad_gt_seg.pkl? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!unzip ../../data/data.zip -d ../../data\n",
    "!unzip ../../data/finetune.zip -d ../../data\n",
    "!unzip ../../data/memory.zip -d ../../data\n",
    "!unzip ../../data/metrics.zip -d ../../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**A Language Agent for Autonomous Driving**\n",
      "Role: You are the brain of an autonomous vehicle (a.k.a. ego-vehicle). In this step, you need to extract necessary information from the driving scenario. The information you extracted must be useful to the next-step motion planning. \n",
      "\n",
      "Necessary information might include the following:\n",
      "- Detections: The detected objects that you need to pay attention to.\n",
      "- Predictions: The estimated future motions of the detected objects. \n",
      "- Maps: Map information includes traffic lanes and road boundaries.\n",
      "- Occunpancy: Occupancy implies whether a location has been occupied by other objects.\n",
      "\n",
      "Task\n",
      "- You should think about what types of information (Detections, Predictions, Maps, Occupancy) you need to extract from the driving scenario.\n",
      "- Detections and Predictions are quite important for motion planning. You should call at least one of them if necessary.\n",
      "- Maps information are also important. You should pay more attention to road shoulder and lane divider information to your current ego-vehicle location.\n",
      "- I will guide you through the thinking process step by step.\n",
      "\n",
      "ego_prompts\n",
      "\n",
      "{'role': 'assistant', 'content': 'YES'}\n",
      "You can execute one of the following functions to get object detection results (don't execute functions that have been used before):\n",
      "- get_leading_object_detection() #Get the detection of the leading object, the function will return the leading object id and its position and size. If there is no leading object, return None\n",
      "- get_object_detections_in_range() #Get the detections of the objects in a given range (x_start, x_end)*(y_start, y_end)m^2, the function will return a list of object ids and their positions and sizes. If there is no object, return None\n",
      "- get_surrounding_object_detections() #Get the detections of the surrounding objects in a 20m*20m range, the function will return a list of surroundind object ids and their positions and sizes. If there is no surrounding object, return None\n",
      "- get_front_object_detections() #Get the detections of the objects in front of you in a 10m*20m range, the function will return a list of front object ids and their positions and sizes. If there is no front object, return None\n",
      "- get_all_object_detections() #Get the detections of all objects in the whole scene, the function will return a list of object ids and their positions and sizes. Always avoid using this function if there are other choices.\n",
      "\n",
      "{'role': 'assistant', 'content': 'YES'}\n",
      "You can execute one of the following functions to get object future trajectory predictions (don't execute functions that have been used before):\n",
      "- get_leading_object_future_trajectory() #Get the predicted future trajectory of the leading object, the function will return a trajectory containing a series of waypoints. If there is no leading vehicle, return None\n",
      "- get_future_trajectories_for_specific_objects() #Get the future trajectories of specific objects (specified by a List of object ids), the function will return trajectories for each object. If there is no object, return None\n",
      "- get_future_trajectories_in_range() #Get the future trajectories where any waypoint in this trajectory falls into a given range (x_start, x_end)*(y_start, y_end)m^2, the function will return each trajectory that satisfies the condition. If there is no trajectory satisfied, return None\n",
      "- get_future_waypoint_of_specific_objects_at_timestep() #Get the future waypoints of specific objects at a specific timestep, the function will return a list of waypoints. If there is no object or the object does not have a waypoint at the given timestep, return None\n",
      "- get_all_future_trajectories() #Get the predicted future trajectories of all objects in the whole scene, the function will return a list of object ids and their future trajectories. Always avoid using this function if there are other choices.\n",
      "\n",
      "{'role': 'assistant', 'content': 'YES'}\n",
      "You can execute one of the following functions to get occupancy information (don't execute functions that have been used before):\n",
      "- get_occupancy_at_locations_for_timestep() #Get the probability whether a list of locations [(x_1, y_1), ..., (x_n, y_n)] is occupied at the timestep t. If the location is out of the occupancy prediction scope, return None\n",
      "\n",
      "{'role': 'assistant', 'content': 'YES'}\n",
      "You can execute one of the following functions to get map information (don't execute functions that have been used before):\n",
      "- get_drivable_at_locations() #Get the drivability at the locations [(x_1, y_1), ..., (x_n, y_n)]. If the location is out of the map scope, return None\n",
      "- get_lane_category_at_locations() #Get the lane category at the locations [(x_1, y_1), ..., (x_n, y_n)]. If the location is out of the map scope, return None\n",
      "- get_distance_to_shoulder_at_locations() #Get the distance to both sides of road shoulders at the locations [(x_1, y_1), ..., (x_n, y_n)]. If the location is out of the map scope, return None\n",
      "- get_current_shoulder() #Get the distance to both sides of road shoulders for the current ego-vehicle location.\n",
      "- get_distance_to_lane_divider_at_locations() #Get the distance to both sides of road lane_dividers at the locations [(x_1, y_1), ..., (x_n, y_n)]. If the location is out of the map scope, return None\n",
      "- get_current_lane_divider() #Get the distance to both sides of road lane_dividers for the current ego-vehicle location\n",
      "- get_nearest_pedestrian_crossing() #Get the location of the nearest pedestrian crossing to the ego-vehicle. If there is no such pedestrian crossing, return None\n",
      "\n",
      "\n",
      "**A Language Agent for Autonomous Driving**\n",
      "Role: You are the brain of an autonomous vehicle (a.k.a. ego-vehicle). In this step, you need to retrieve the most similar past driving experience to help decision-making.\n",
      "\n",
      "Task\n",
      "- You will receive the current driving scenario.\n",
      "- You will also receive several past driving experiences.\n",
      "- You should decide ONLY ONE experience that is most similar to the current scenario based on the information provided.\n",
      "- Please answer ONLY the index (e.g., 0, 1, 2) of the most similar experience.\n",
      "\n",
      "** Current Scenario: **:\n",
      "ego_promptsFound 3 relevant experiences:\n",
      "** Past Driving Experience 1: **\n",
      "*****Past Ego States:*****\n",
      "Current State:\n",
      " - Velocity (vx,vy): (0.00,0.03)\n",
      " - Heading Angular Velocity (v_yaw): (0.01)\n",
      " - Acceleration (ax,ay): (0.13,0.67)\n",
      " - Can Bus: (0.43,-0.24)\n",
      " - Heading Speed: (0.04)\n",
      " - Steering: (1.16)\n",
      "Historical Trajectory (last 2 seconds): [(0.32,1.20), (0.24,0.93), (0.14,0.60), (0.00,-0.03)]\n",
      "Mission Goal: FORWARD\n",
      "** Past Driving Experience 2: **\n",
      "*****Past Ego States:*****\n",
      "Current State:\n",
      " - Velocity (vx,vy): (-0.00,0.09)\n",
      " - Heading Angular Velocity (v_yaw): (0.00)\n",
      " - Acceleration (ax,ay): (0.00,0.10)\n",
      " - Can Bus: (0.96,0.28)\n",
      " - Heading Speed: (0.23)\n",
      " - Steering: (0.32)\n",
      "Historical Trajectory (last 2 seconds): [(-0.03,-0.12), (-0.02,-0.06), (-0.02,-0.08), (-0.01,-0.09)]\n",
      "Mission Goal: FORWARD\n",
      "** Past Driving Experience 3: **\n",
      "*****Past Ego States:*****\n",
      "Current State:\n",
      " - Velocity (vx,vy): (0.00,0.06)\n",
      " - Heading Angular Velocity (v_yaw): (0.00)\n",
      " - Acceleration (ax,ay): (-0.00,0.06)\n",
      " - Can Bus: (1.43,-0.02)\n",
      " - Heading Speed: (0.30)\n",
      " - Steering: (0.91)\n",
      "Historical Trajectory (last 2 seconds): [(0.00,-0.05), (0.00,-0.05), (0.00,-0.05), (0.00,-0.05)]\n",
      "Mission Goal: FORWARD\n",
      "Please return the index 1-3 of the most similar experience: \n",
      "Memory-GPT response: After analyzing the current scenario and past driving experiences, I would recommend:\n",
      "\n",
      "**Index: 2**\n",
      "\n",
      "This experience is the most similar to the current scenario based on the ego states (velocity, heading angular velocity, acceleration, CAN bus, heading speed, and steering) and historical trajectory. The mission goal of FORWARD in both scenarios also matches.\n",
      "*****Chain of Thoughts Reasoning:*****\n",
      "It seems like you're trying to provide input for an autonomous vehicle's language agent! You've got two sets of prompts:\n",
      "\n",
      "**Ego-Prompts**\n",
      "\n",
      "These are the current ego-states, which include:\n",
      "\n",
      "1. Current State:\n",
      "\t* Velocity (vx, vy): (-0.02, 2.66)\n",
      "\t* Heading Angular Velocity (v_yaw): (-0.01)\n",
      "\t* Acceleration (ax, ay): (0.00, 0.00)\n",
      "\t* Can Bus: (-1.72, -0.95)\n",
      "\t* Heading Speed: (2.83)\n",
      "\t* Steering: (1.12)\n",
      "2. Historical Trajectory (last 2 seconds): [(-1.16, -10.63), (-0.87, -7.97), (-0.58, -5.32), (-0.29, -2.66)]\n",
      "3. Mission Goal: RIGHT\n",
      "\n",
      "**Perception-Prompts**\n",
      "\n",
      "These are the current perception results, which include:\n",
      "\n",
      "1. Front object detections:\n",
      "\t* Object type: bicycle\n",
      "\t* Object id: 0\n",
      "\t* Position: (-1.02, 7.49)\n",
      "\t* Size: (0.49, 1.67)\n",
      "2. Future trajectories for specific objects:\n",
      "\t* Object type: bicycle\n",
      "\t* Object id: 0\n",
      "\t* Future waypoint coordinates in 3s: [(-1.02, 7.51), (-1.02, 7.52), (-1.02, 7.54), (-1.03, 7.55), (-1.02, 7.59), (-1.02, 7.61)]\n",
      "\t* Object type: car\n",
      "\t* Object id: 1\n",
      "\t* Future waypoint coordinates in 3s: [(8.71, 18.66), (8.70, 18.65), (8.69, 18.65), (8.69, 18.64), (8.69, 18.63), (8.69, 18.65)]\n",
      "3. Distance to both sides of road shoulders of current ego-vehicle location:\n",
      "\t* Current ego-vehicle's distance to left shoulder: 1.0m\n",
      "\t* Current ego-vehicle's distance to right shoulder: 0.5m\n",
      "\n",
      "Please provide the expected output for these prompts!\n",
      "Planned Trajectory:\n",
      "[(0.01,0.74), (0.03,1.58), (0.04,2.50), (0.06,3.54), (0.08,4.91), (0.10,6.31)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Ollama/Agent-Driver/agentdriver/unit_test/../../agentdriver/planning/planning_agent.py:19: UserWarning: Input motion planning model might not be correct,                   expect a fintuned model like ft:gpt-3.5-turbo-0613:your_org::your_model_id,                   but get llama3\n",
      "  warnings.warn(f\"Input motion planning model might not be correct, \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0a0d6b8c2e884134a3b48df43d54c36a\n",
      "GPT  Planner:\n",
      " Based on the provided inputs, I will plan a safe and feasible 3-second trajectory of 6 waypoints.\n",
      "\n",
      "Planned Trajectory:\n",
      "[(-1.02, 7.51), (-1.01, 7.52), (-0.99, 7.53), (-0.97, 7.54), (-0.95, 7.55), (-0.93, 7.56)]\n",
      "\n",
      "Note: The planned trajectory is designed to avoid collision with the detected bicycle and maintain a safe distance from it while also considering the ego-vehicle's current state and historical trajectory.\n",
      "Ground Truth:\n",
      " Planned Trajectory:\n",
      "[(0.01,0.74), (0.03,1.58), (0.04,2.50), (0.06,3.54), (0.08,4.91), (0.10,6.31)]\n",
      "Extracted traj string: '[(-1.02, 7.51), (-1.01, 7.52), (-0.99, 7.53), (-0.97, 7.54), (-0.95, 7.55), (-0.93, 7.56)]'\n",
      "0a0d6b8c2e884134a3b48df43d54c36a\n",
      "ego_prompts\n",
      "perception_prompts\n",
      "\n",
      "*****Traffic Rules:*****\n",
      "- Avoid collision with other objects.\n",
      "- Always drive on drivable regions.\n",
      "- Avoid driving on occupied regions.\n",
      "- Pay attention to your ego-states and historical trajectory when planning.\n",
      "- Maintain a safe distance from the objects in front of you.\n",
      "\n",
      "None\n",
      "*****Chain of Thoughts Reasoning:*****\n",
      "It seems like you're trying to provide input for an autonomous vehicle's language agent! You've got two sets of prompts:\n",
      "\n",
      "**Ego-Prompts**\n",
      "\n",
      "These are the current ego-states, which include:\n",
      "\n",
      "1. Current State:\n",
      "\t* Velocity (vx, vy): (-0.02, 2.66)\n",
      "\t* Heading Angular Velocity (v_yaw): (-0.01)\n",
      "\t* Acceleration (ax, ay): (0.00, 0.00)\n",
      "\t* Can Bus: (-1.72, -0.95)\n",
      "\t* Heading Speed: (2.83)\n",
      "\t* Steering: (1.12)\n",
      "2. Historical Trajectory (last 2 seconds): [(-1.16, -10.63), (-0.87, -7.97), (-0.58, -5.32), (-0.29, -2.66)]\n",
      "3. Mission Goal: RIGHT\n",
      "\n",
      "**Perception-Prompts**\n",
      "\n",
      "These are the current perception results, which include:\n",
      "\n",
      "1. Front object detections:\n",
      "\t* Object type: bicycle\n",
      "\t* Object id: 0\n",
      "\t* Position: (-1.02, 7.49)\n",
      "\t* Size: (0.49, 1.67)\n",
      "2. Future trajectories for specific objects:\n",
      "\t* Object type: bicycle\n",
      "\t* Object id: 0\n",
      "\t* Future waypoint coordinates in 3s: [(-1.02, 7.51), (-1.02, 7.52), (-1.02, 7.54), (-1.03, 7.55), (-1.02, 7.59), (-1.02, 7.61)]\n",
      "\t* Object type: car\n",
      "\t* Object id: 1\n",
      "\t* Future waypoint coordinates in 3s: [(8.71, 18.66), (8.70, 18.65), (8.69, 18.65), (8.69, 18.64), (8.69, 18.63), (8.69, 18.65)]\n",
      "3. Distance to both sides of road shoulders of current ego-vehicle location:\n",
      "\t* Current ego-vehicle's distance to left shoulder: 1.0m\n",
      "\t* Current ego-vehicle's distance to right shoulder: 0.5m\n",
      "\n",
      "Please provide the expected output for these prompts!\n",
      "(array([[-1.02,  7.51],\n",
      "       [-1.01,  7.52],\n",
      "       [-0.99,  7.53],\n",
      "       [-0.97,  7.54],\n",
      "       [-0.95,  7.55],\n",
      "       [-0.93,  7.56]]), {'token': '0a0d6b8c2e884134a3b48df43d54c36a', 'Prediction': \"Based on the provided inputs, I will plan a safe and feasible 3-second trajectory of 6 waypoints.\\n\\nPlanned Trajectory:\\n[(-1.02, 7.51), (-1.01, 7.52), (-0.99, 7.53), (-0.97, 7.54), (-0.95, 7.55), (-0.93, 7.56)]\\n\\nNote: The planned trajectory is designed to avoid collision with the detected bicycle and maintain a safe distance from it while also considering the ego-vehicle's current state and historical trajectory.\", 'Ground Truth': 'Planned Trajectory:\\n[(0.01,0.74), (0.03,1.58), (0.04,2.50), (0.06,3.54), (0.08,4.91), (0.10,6.31)]'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-1.02,  7.51],\n",
       "        [-1.01,  7.52],\n",
       "        [-0.99,  7.53],\n",
       "        [-0.97,  7.54],\n",
       "        [-0.95,  7.55],\n",
       "        [-0.93,  7.56]]),\n",
       " {'token': '0a0d6b8c2e884134a3b48df43d54c36a',\n",
       "  'Prediction': \"Based on the provided inputs, I will plan a safe and feasible 3-second trajectory of 6 waypoints.\\n\\nPlanned Trajectory:\\n[(-1.02, 7.51), (-1.01, 7.52), (-0.99, 7.53), (-0.97, 7.54), (-0.95, 7.55), (-0.93, 7.56)]\\n\\nNote: The planned trajectory is designed to avoid collision with the detected bicycle and maintain a safe distance from it while also considering the ego-vehicle's current state and historical trajectory.\",\n",
       "  'Ground Truth': 'Planned Trajectory:\\n[(0.01,0.74), (0.03,1.58), (0.04,2.50), (0.06,3.54), (0.08,4.91), (0.10,6.31)]'})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from agentdriver.main.language_agent import LanguageAgent\n",
    "from agentdriver.llm_core.api_keys import OPENAI_ORG, OPENAI_API_KEY, FINETUNE_PLANNER_NAME\n",
    "\n",
    "import openai\n",
    "openai.organization = OPENAI_ORG\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "FINETUNE_PLANNER_NAME = \"llama3\" \n",
    "\n",
    "data_path = Path('../../data/')\n",
    "split = 'val'\n",
    "# language_agent = LanguageAgent(data_path, split, planner_model_name=FINETUNE_PLANNER_NAME, verbose=True)\n",
    "language_agent = LanguageAgent(\n",
    "    data_path, split,\n",
    "    planner_model_name=FINETUNE_PLANNER_NAME,\n",
    "    verbose=True,\n",
    "    model_name=\"llama3\",   \n",
    "    backend=\"ollama\"\n",
    ")\n",
    "\n",
    "token = \"0a0d6b8c2e884134a3b48df43d54c36a\"\n",
    "language_agent.inference_single(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
